{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Floating Point Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the following we learn how computers represent floating point numbers. Consider for example the number\n",
    "$1/3 = 0.33333...$. Computers only have a finite amount of memory. We could try to work directly with fractional\n",
    "expressions such as $1/3$ and keep track of nominators and denominators. But it turns out that the number of digits on\n",
    "both nominator and denominator tends to grow enormously as a computation progresses, making this approach infeasible.\n",
    "\n",
    "On the other hand, if we want to store the sequence of digits $0.33333...$ we have to cut-off at some point as computers\n",
    "only have a finite amount of memory. Hence, we are introducing a small error. This cut-off and the resulting error\n",
    "are rigorously defined in the IEEE 754 standard, which describes how to represent floating point numbers on modern computers.\n",
    "This allows us to prove rigorous theorems that take into account that computers only support finite precision arithmetic.\n",
    "\n",
    "The two most commonly types for floating point numbers are IEEE double precision and IEEE single precision. The former gives\n",
    "us around 15 digits of accuracy. The latter around 7 digits of accuracy. For almost all purposes this is more than enough. Take for example the gravitational constant $G$. It is known to around $4$ digits of accuracy, only\n",
    "a fraction of the number of digits we have available in modern computers.\n",
    "\n",
    "For most numerical calculations double precision is preferred. The reason is that in some computations errors might accumulate, leading to loss of accuracy. In double precision we have much more headroom for this than in single precision.\n",
    "\n",
    "However, not all applications need such precision. For example, many\n",
    "machine learning applications use the half-precision type that is even less\n",
    "accurate than single precision but can be evaluated extremely efficiently on\n",
    "dedicated hardware (e.g. tensore cores on modern machine learning accelerators).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Floating point types in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Numpy module defines convenient ways to query properties of floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # import the numpy extension module\n",
    "                   # and call it np as short form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The data type names in `Numpy` for floating point types are:\n",
    "\n",
    "* `IEEE double precision`: np.float, np.double, np.float64\n",
    "* `IEEE single precision`: np.single, np.float32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us query some properties of these numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "double_precision_info = np.finfo(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The biggest and smallest (by absolute value) normalized floating point numbers are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7976931348623157e+308"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_precision_info.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2250738585072014e-308"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_precision_info.tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Floating point numbers can not be arbitrarily close to each other. There is a smallest relative distance, which we define shortly. It is given as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_precision_info.eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This leads to a limited precision of floating point numbers. The approximate relative precision is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_precision_info.precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Task: What are the values for single precision arithmetic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4028235e+38, 1.1754944e-38, 1.1920929e-07, 6]\n"
     ]
    }
   ],
   "source": [
    "single_precision_info = np.finfo(np.float32)\n",
    "print([single_precision_info.max,\n",
    "       single_precision_info.tiny,\n",
    "       single_precision_info.eps,\n",
    "       single_precision_info.precision])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definition of floating point numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We use the following model for floating point numbers. For more details, see e.g.\n",
    "the recommended lecture book by Trefethen and Bau.\n",
    "\n",
    "\n",
    "The set of floating point numbers is defined as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{F} = \\left\\{(-1)^s\\cdot b^e \\cdot \\frac{m}{b^{p-1}} :\\right.\n",
    "\\left. s = 0,1; e_{min}\\leq e \\leq e_{max}; b^{p-1}\\leq m\\leq b^{p}-1\\right\\}.\n",
    "$$\n",
    "\n",
    "* `IEEE double precision`: $e_{min} = -1022, e_{max} = 1023, p = 53$\n",
    "* `IEEE single precision`: $e_{min} = -126, e_{max} = 127, p = 24$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Typically, b = 2. For the mantissa we have:\n",
    "\n",
    "$$\n",
    "\\frac{m}{b^{p-1}} = 1, 1+b^{1-p}, 1+2b^{1-p}, \\dots, b-b^{1-p}\n",
    "$$\n",
    "\n",
    "$\\rightarrow$ Distance of neighbouring floats is $2^e b^{1-p}$.\n",
    "\n",
    "$\\epsilon_{rel} = b^{1-p}$ is smallest number such that\n",
    "$$\n",
    "1 + \\epsilon_{rel} \\neq 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + double_precision_info.eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + .25 * double_precision_info.eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximating numbers in floating point arithmetic\n",
    "Let $x\\in\\mathbb{R}$, $b^{e_{min}}\\leq x < b^{e_{max}+1}$. Define $\\epsilon_{mach}:=\\frac{1}{2}b^{1-p}$\n",
    "\n",
    "There exists $x'\\in \\mathcal{F}$ such that $|x-x'|\\leq\\epsilon_{mach}|x|$.\n",
    "\n",
    "**$\\epsilon_{mach}$ is relative distance to the next floating point number in $\\mathcal{F}$.**\n",
    "\n",
    "Define the projection\n",
    "\n",
    "$$\n",
    "fl:fl(x)\\rightarrow x',\n",
    "$$\n",
    "\n",
    "where $x'$ is the closest floating point number in $\\mathcal{F}$. \n",
    "\n",
    "It follows that $fl(x)=x*(1+\\epsilon)$ for some $|\\epsilon|\\leq \\epsilon_{mach}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fundamental Axiom of Floating Point Arithmetic\n",
    "Define $x\\odot y = fl(x \\cdot y)$, where $\\cdot$ is one of $+,-,\\times,\\div$. Then for all $x,y\\in\\mathcal{F}$ there exists $\\epsilon$ with $|\\epsilon| \\leq \\epsilon_{mach}$ such that\n",
    "\n",
    "$$ \n",
    "x\\odot y = (x \\cdot y)(1+\\epsilon).\n",
    "$$\n",
    "\n",
    "Most modern architectures guarantee this property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Special symbols in floating point arithmetic\n",
    "\n",
    "In addition to numbers several other important symbols are defined in the floating point standard. The most important are:\n",
    "\n",
    "* NaN: Not a number\n",
    "* $\\pm$ inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7918/3058898986.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  b = np.float64(0) / np.float64(0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.inf\n",
    "b = np.float64(0) / np.float64(0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above we have explicitly converted the zeros in the division $0/0$ to use the corresponding Numpy type. The reason is that Python itself is not fully IEEE 754 compliant. According to the standard the division $0 / 0$ is valid and has *nan* as result. But let's consider what Python is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7918/617932585.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.inf\n",
    "b = 0. / 0.\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the result *nan* Python returns a runtime error. This may be useful for certain applications but not for numerics and is not IEEE 754 compliant. This is one of many reasons why for numerical computations the `Numpy` module is so important for Python."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "42014f68de5ec8dd1b986d57ba13c34861df495ba716d30160e7c663f4569539"
  },
  "kernelspec": {
   "display_name": "Python [conda env:dev]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
